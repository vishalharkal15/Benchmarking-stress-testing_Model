#!/usr/bin/env python3
"""
Deep Learning Hardware Capability Test
Tests system performance for deep learning workloads including:
- GPU/CPU detection and configuration
- Large neural network training
- Model optimization and tuning
- Memory handling
"""

import time
import sys
import platform
import psutil
import numpy as np

print("="*80)
print("DEEP LEARNING HARDWARE CAPABILITY TEST")
print("="*80)

# ============================================================================
# STEP 1: System Information
# ============================================================================
print("\n[1/6] SYSTEM INFORMATION")
print("-"*80)
print(f"OS: {platform.system()} {platform.release()}")
print(f"Python Version: {sys.version.split()[0]}")
print(f"CPU: {platform.processor()}")
print(f"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical")
print(f"RAM: {psutil.virtual_memory().total / (1024**3):.2f} GB")
print(f"Available RAM: {psutil.virtual_memory().available / (1024**3):.2f} GB")

# ============================================================================
# STEP 2: Import Deep Learning Libraries
# ============================================================================
print("\n[2/6] IMPORTING DEEP LEARNING LIBRARIES")
print("-"*80)

try:
    import tensorflow as tf
    print(f"âœ“ TensorFlow version: {tf.__version__}")
    
    # Check GPU availability
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"âœ“ GPUs detected: {len(gpus)}")
        for i, gpu in enumerate(gpus):
            print(f"  - GPU {i}: {gpu.name}")
            # Enable memory growth
            try:
                tf.config.experimental.set_memory_growth(gpu, True)
                print(f"    Memory growth enabled for GPU {i}")
            except RuntimeError as e:
                print(f"    Warning: {e}")
    else:
        print("âš  No GPU detected - will use CPU (slower)")
    
    # Check available devices
    print(f"\nAvailable devices:")
    for device in tf.config.list_logical_devices():
        print(f"  - {device.device_type}: {device.name}")
        
except ImportError as e:
    print(f"âœ— TensorFlow import failed: {e}")
    print("Please install: pip install tensorflow")
    sys.exit(1)

try:
    from tensorflow import keras
    from tensorflow.keras import layers, models, optimizers
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    print("âœ“ Keras imported successfully")
except ImportError as e:
    print(f"âœ— Keras import failed: {e}")
    sys.exit(1)

# ============================================================================
# STEP 3: Create High-Performance Neural Network
# ============================================================================
print("\n[3/6] BUILDING COMPLEX NEURAL NETWORK")
print("-"*80)

def create_advanced_model(input_shape=(224, 224, 3), num_classes=1000):
    """
    Creates a deep convolutional neural network similar to ResNet architecture
    This is a compute-intensive model to test system capabilities
    """
    inputs = keras.Input(shape=input_shape)
    
    # Initial convolution
    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)
    
    # Residual blocks
    def residual_block(x, filters, downsample=False):
        identity = x
        strides = 2 if downsample else 1
        
        x = layers.Conv2D(filters, 3, strides=strides, padding='same')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        
        x = layers.Conv2D(filters, 3, padding='same')(x)
        x = layers.BatchNormalization()(x)
        
        if downsample or identity.shape[-1] != filters:
            identity = layers.Conv2D(filters, 1, strides=strides, padding='same')(identity)
            identity = layers.BatchNormalization()(identity)
        
        x = layers.Add()([x, identity])
        x = layers.Activation('relu')(x)
        return x
    
    # Stack of residual blocks (deep network)
    filters_list = [64, 128, 256, 512]
    blocks_per_stage = [3, 4, 6, 3]
    
    for stage, (filters, num_blocks) in enumerate(zip(filters_list, blocks_per_stage)):
        for block in range(num_blocks):
            downsample = (block == 0 and stage > 0)
            x = residual_block(x, filters, downsample)
    
    # Classification head
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    
    model = keras.Model(inputs=inputs, outputs=outputs, name='DeepResNet_Test')
    return model

# Create model
model = create_advanced_model(input_shape=(112, 112, 3), num_classes=10)
print(f"âœ“ Model created: {model.name}")
print(f"âœ“ Total parameters: {model.count_params():,}")
print(f"âœ“ Total layers: {len(model.layers)}")

# Display model summary
print("\nModel Architecture Summary:")
model.summary(line_length=80)

# ============================================================================
# STEP 4: Compile Model with Optimization
# ============================================================================
print("\n[4/6] COMPILING MODEL WITH OPTIMIZATIONS")
print("-"*80)

# Advanced optimizer with learning rate scheduling
initial_learning_rate = 0.001
optimizer = optimizers.Adam(
    learning_rate=initial_learning_rate,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07
)

model.compile(
    optimizer=optimizer,
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']  # Removed top_k_categorical_accuracy due to compatibility issues
)
print("âœ“ Model compiled with Adam optimizer")
print(f"âœ“ Initial learning rate: {initial_learning_rate}")
print("âœ“ Loss function: Sparse Categorical Crossentropy")
print("âœ“ Metrics: Accuracy, Top-K Accuracy")

# ============================================================================
# STEP 5: Generate Synthetic Data and Train
# ============================================================================
print("\n[5/6] GENERATING SYNTHETIC DATA AND TRAINING")
print("-"*80)

# Generate synthetic training data
print("Generating synthetic training data...")
batch_size = 16  # Reduced batch size
num_train_samples = 200  # Reduced dataset size
num_val_samples = 50

# Create random data (simulating ImageNet-like dataset)
X_train = np.random.random((num_train_samples, 112, 112, 3)).astype('float32')  # Smaller images
y_train = np.random.randint(0, 10, num_train_samples).astype('int32')  # Fewer classes

X_val = np.random.random((num_val_samples, 112, 112, 3)).astype('float32')
y_val = np.random.randint(0, 10, num_val_samples).astype('int32')

print(f"âœ“ Training data: {X_train.shape}")
print(f"âœ“ Validation data: {X_val.shape}")
print(f"âœ“ Batch size: {batch_size}")

# Setup callbacks for optimization
callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=2,
        min_lr=1e-7,
        verbose=1
    )
]

print("\n" + "="*80)
print("STARTING TRAINING - HARDWARE STRESS TEST")
print("="*80)

# Record start time and memory
start_time = time.time()
start_memory = psutil.virtual_memory().used / (1024**3)

try:
    # Train the model
    history = model.fit(
        X_train, y_train,
        batch_size=batch_size,
        epochs=5,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )
    
    # Record end time and memory
    end_time = time.time()
    end_memory = psutil.virtual_memory().used / (1024**3)
    training_time = end_time - start_time
    memory_used = end_memory - start_memory
    
    print("\n" + "="*80)
    print("TRAINING COMPLETED SUCCESSFULLY")
    print("="*80)
    print(f"âœ“ Total training time: {training_time:.2f} seconds")
    print(f"âœ“ Average time per epoch: {training_time / len(history.history['loss']):.2f} seconds")
    print(f"âœ“ Memory usage: {memory_used:.2f} GB")
    print(f"âœ“ Final training accuracy: {history.history['accuracy'][-1]:.4f}")
    print(f"âœ“ Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}")
    
except Exception as e:
    print(f"\nâœ— Training failed: {e}")
    import traceback
    traceback.print_exc()
    training_time = 0  # Set default value for assessment

# ============================================================================
# STEP 6: Performance Benchmarking
# ============================================================================
print("\n[6/6] PERFORMANCE BENCHMARKING")
print("-"*80)

# Inference speed test
print("\nTesting inference speed...")
test_batch = np.random.random((batch_size, 112, 112, 3)).astype('float32')

# Warm-up
_ = model.predict(test_batch, verbose=0)

# Benchmark
num_iterations = 100
inference_times = []

for i in range(num_iterations):
    start = time.time()
    _ = model.predict(test_batch, verbose=0)
    inference_times.append(time.time() - start)

avg_inference_time = np.mean(inference_times)
throughput = batch_size / avg_inference_time

print(f"âœ“ Average inference time: {avg_inference_time*1000:.2f} ms")
print(f"âœ“ Throughput: {throughput:.2f} images/second")
print(f"âœ“ Inference std dev: {np.std(inference_times)*1000:.2f} ms")

# ============================================================================
# FINAL RESULTS
# ============================================================================
print("\n" + "="*80)
print("FINAL SYSTEM ASSESSMENT")
print("="*80)

# Calculate performance score
performance_score = 0
if gpus:
    performance_score += 40
    print("âœ“ GPU Available: +40 points")
else:
    print("âš  No GPU: +0 points (CPU only)")

if training_time < 60:
    performance_score += 30
    print(f"âœ“ Fast Training (<60s): +30 points")
elif training_time < 120:
    performance_score += 20
    print(f"âš  Moderate Training (60-120s): +20 points")
else:
    performance_score += 10
    print(f"âš  Slow Training (>120s): +10 points")

if throughput > 100:
    performance_score += 30
    print(f"âœ“ High Throughput (>100 img/s): +30 points")
elif throughput > 50:
    performance_score += 20
    print(f"âš  Good Throughput (50-100 img/s): +20 points")
else:
    performance_score += 10
    print(f"âš  Low Throughput (<50 img/s): +10 points")

print(f"\n{'='*80}")
print(f"FINAL SCORE: {performance_score}/100")
print(f"{'='*80}")

if performance_score >= 80:
    print("ðŸŒŸ EXCELLENT: System is highly capable for deep learning!")
    print("   Your hardware can handle production-level ML workloads.")
elif performance_score >= 60:
    print("âœ“ GOOD: System is suitable for deep learning development.")
    print("  Your hardware can handle most ML tasks with decent performance.")
elif performance_score >= 40:
    print("âš  FAIR: System can run deep learning but may be slow.")
    print("  Consider upgrading GPU for better performance.")
else:
    print("âš  LIMITED: System has basic deep learning capability.")
    print("  Recommend GPU upgrade for serious ML work.")

print("\n" + "="*80)
print("TEST COMPLETED")
print("="*80)
